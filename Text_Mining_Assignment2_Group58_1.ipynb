{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining - Assignment 2: Sequence Labelling\n",
    "## Group 58: Vasiliki Gkika, Pelagia Kalpakidou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import re\n",
    "import datasets\n",
    "from datasets import DatasetDict\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and convert IOB file to correct data structure\n",
    "def read_datasets(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        raw_text = file.read().strip()\n",
    "    \n",
    "    raw_docs = re.split(r'\\n\\t?\\n', raw_text)\n",
    "    token_docs = []\n",
    "    tag_docs = []\n",
    "   \n",
    "    for doc in raw_docs:\n",
    "        tokens = []\n",
    "        tags = []\n",
    "        for line in doc.split('\\n'):\n",
    "            if len(line.split('\\t')) < 2:\n",
    "                continue\n",
    "            token, tag = line.split('\\t')\n",
    "            tokens.append(token)\n",
    "            tags.append(tag)\n",
    "        token_docs.append(tokens)\n",
    "        tag_docs.append(tags)\n",
    "\n",
    "    return token_docs, tag_docs\n",
    "\n",
    "tokens_train, tag_train = read_datasets('W-NUT_data/wnut17train.conll')\n",
    "tokens_dev, tag_dev = read_datasets('W-NUT_data/emerging.dev.conll')\n",
    "tokens_test, tag_test = read_datasets('W-NUT_data/emerging.test.annotated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map IOB tags to NER tags\n",
    "mapping = {\n",
    "        'O': 0,\n",
    "        'B-corporation': 1,\n",
    "        'I-corporation': 2,\n",
    "        'B-creative-work': 3,\n",
    "        'I-creative-work': 4,\n",
    "        'B-group': 5,\n",
    "        'I-group': 6,\n",
    "        'B-location': 7,\n",
    "        'I-location': 8,\n",
    "        'B-person': 9,\n",
    "        'I-person': 10,\n",
    "        'B-product': 11,\n",
    "        'I-product': 12,\n",
    "    }\n",
    "\n",
    "def IOB_to_NER (tokens, iob_tags):\n",
    "    ner_tags = []\n",
    "    for iob in iob_tags:\n",
    "        ner_tags.append([mapping[tag] for tag in iob])\n",
    "    return ner_tags\n",
    "\n",
    "ner_train = IOB_to_NER(tokens_train, tag_train)\n",
    "ner_dev = IOB_to_NER(tokens_dev, tag_dev)\n",
    "ner_test = IOB_to_NER(tokens_test, tag_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3394"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = datasets.Dataset.from_dict({\"id\": range(len(tokens_train)), \"tokens\": tokens_train, \"iob_tags\": tag_train, \"ner_tags\": ner_train})\n",
    "validation_dataset = datasets.Dataset.from_dict({\"id\": range(len(tokens_dev)), \"tokens\": tokens_dev, \"iob_tags\": tag_dev, \"ner_tags\": ner_dev})\n",
    "test_dataset = datasets.Dataset.from_dict({\"id\": range(len(tokens_test)), \"tokens\": tokens_test, \"iob_tags\": tag_test, \"ner_tags\": ner_test})\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "combined_datasets = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": validation_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "combined_datasets\n",
    "len(combined_datasets[\"train\"][\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoding and displaying the NER tags in a human-readable format\n",
    "words = combined_datasets[\"train\"][0][\"tokens\"]\n",
    "labels = combined_datasets[\"train\"][0][\"iob_tags\"]\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "\n",
    "for word, labels in zip(words, labels):\n",
    "    max_length = max(len(word), max(len(label) for label in labels))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += \" \".join(labels) + \" \" * (max_length - max(len(label) for label in labels) + 1)\n",
    "\n",
    "print(line1)\n",
    "print(line2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Align labels with tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# tokenize a pre-tokenized input\n",
    "inputs = tokenizer(combined_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
    "print(inputs.tokens(), )\n",
    "print(inputs.word_ids(), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "    return new_labels\n",
    "\n",
    "\n",
    "labels = combined_datasets[\"train\"][0][\"ner_tags\"]\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c208234f20946588d34b162d0b50e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c522b2667d6b4fe895af2773bfea28d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1009 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d04d15a9db3b4352b5fceca9e6154079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1287 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3394\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1009\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1287\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "\n",
    "tokenized_datasets = combined_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=combined_datasets[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "# data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer, return_tensors=\"tf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['O', 'B-corporation', 'I-corporation', 'B-creative-work', 'I-creative-work', 'B-group', 'I-group', 'B-location', 'I-location', 'B-person', 'I-person', 'B-product', 'I-product']\n",
    "print(label_names, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\", \"token_type_ids\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    ")\n",
    "\n",
    "tf_eval_dataset = tokenized_datasets[\"validation\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\", \"token_type_ids\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    ")\n",
    "\n",
    "tf_test_dataset = tokenized_datasets[\"test\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\", \"token_type_ids\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForTokenClassification\n",
    "\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ba384791994b19a5bd1180abdcd876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n",
    "\n",
    "# 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "import tensorflow as tf\n",
    "\n",
    "# Train in mixed-precision float16\n",
    "# Comment this line out if you're using a GPU that will not benefit from this\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied\n",
    "# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,\n",
    "# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.\n",
    "num_epochs = 3\n",
    "num_train_steps = len(tf_train_dataset) * num_epochs\n",
    "\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_warmup_steps=0,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    ")\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pkalp\\Desktop\\Text_Mining\\Assignments\\Assignment2\\bert-finetuned-ner is already a clone of https://huggingface.co/PelagiaKalpakidou/bert-finetuned-ner. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "\n",
    "callback = PushToHubCallback(output_dir=\"bert-finetuned-ner\", tokenizer=tokenizer)\n",
    "\n",
    "model.fit(\n",
    "    tf_train_dataset,\n",
    "    # validation_data=tf_eval_dataset,\n",
    "    callbacks=[callback],\n",
    "    epochs=num_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of labels: torch.Size([3394, 90])\n"
     ]
    }
   ],
   "source": [
    "# Use the entire training dataset\n",
    "train_batch = data_collator(tokenized_datasets[\"train\"])\n",
    "\n",
    "# Access the labels in the batch\n",
    "labels = train_batch[\"labels\"]\n",
    "\n",
    "# Print the shape of the labels (just for verification)\n",
    "print(\"Shape of labels:\", labels.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size of training data: 3394\n",
      "Batch size of evaluation data: 1287\n"
     ]
    }
   ],
   "source": [
    "# Check the batch size of the training data\n",
    "batch_size_train = len(tokenized_datasets[\"train\"])\n",
    "print(\"Batch size of training data:\", batch_size_train)\n",
    "\n",
    "# Check the batch size of the evaluation data\n",
    "batch_size_eval = len(tokenized_datasets[\"test\"])\n",
    "print(\"Batch size of evaluation data:\", batch_size_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.637140637140637"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf_validation_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Vicky\\Desktop\\Vicky\\Leiden\\Leiden University\\MSc Statistics & Data Science\\3rd Semester\\Text Mining\\Assignment2\\Text_Mining_Assignment2_Group58.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vicky/Desktop/Vicky/Leiden/Leiden%20University/MSc%20Statistics%20%26%20Data%20Science/3rd%20Semester/Text%20Mining/Assignment2/Text_Mining_Assignment2_Group58.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlosses\u001b[39;00m \u001b[39mimport\u001b[39;00m SparseCategoricalCrossentropy\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vicky/Desktop/Vicky/Leiden/Leiden%20University/MSc%20Statistics%20%26%20Data%20Science/3rd%20Semester/Text%20Mining/Assignment2/Text_Mining_Assignment2_Group58.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vicky/Desktop/Vicky/Leiden/Leiden%20University/MSc%20Statistics%20%26%20Data%20Science/3rd%20Semester/Text%20Mining/Assignment2/Text_Mining_Assignment2_Group58.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     optimizer\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39madam\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vicky/Desktop/Vicky/Leiden/Leiden%20University/MSc%20Statistics%20%26%20Data%20Science/3rd%20Semester/Text%20Mining/Assignment2/Text_Mining_Assignment2_Group58.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     loss\u001b[39m=\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vicky/Desktop/Vicky/Leiden/Leiden%20University/MSc%20Statistics%20%26%20Data%20Science/3rd%20Semester/Text%20Mining/Assignment2/Text_Mining_Assignment2_Group58.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     metrics\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vicky/Desktop/Vicky/Leiden/Leiden%20University/MSc%20Statistics%20%26%20Data%20Science/3rd%20Semester/Text%20Mining/Assignment2/Text_Mining_Assignment2_Group58.ipynb#X26sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m )\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vicky/Desktop/Vicky/Leiden/Leiden%20University/MSc%20Statistics%20%26%20Data%20Science/3rd%20Semester/Text%20Mining/Assignment2/Text_Mining_Assignment2_Group58.ipynb#X26sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m model\u001b[39m.\u001b[39mfit(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vicky/Desktop/Vicky/Leiden/Leiden%20University/MSc%20Statistics%20%26%20Data%20Science/3rd%20Semester/Text%20Mining/Assignment2/Text_Mining_Assignment2_Group58.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     tf_train_dataset,\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Vicky/Desktop/Vicky/Leiden/Leiden%20University/MSc%20Statistics%20%26%20Data%20Science/3rd%20Semester/Text%20Mining/Assignment2/Text_Mining_Assignment2_Group58.ipynb#X26sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     validation_data\u001b[39m=\u001b[39mtf_validation_dataset,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vicky/Desktop/Vicky/Leiden/Leiden%20University/MSc%20Statistics%20%26%20Data%20Science/3rd%20Semester/Text%20Mining/Assignment2/Text_Mining_Assignment2_Group58.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf_validation_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_validation_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translating text to numbers is known as encoding. Encoding is done in a two-step process: the tokenization, followed by the conversion to input IDs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
